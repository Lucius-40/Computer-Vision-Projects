{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c3f0f9f",
   "metadata": {},
   "source": [
    "I will do a handwritten digit classification as a starting poihnt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b785aa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7023da74",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # this converts the image to tensor\n",
    "    transforms.Normalize((0.5,),(0.5,)) #mean 0.5 and std dev 0.5\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d504179e",
   "metadata": {},
   "source": [
    "Loading the dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a63b2051",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:07<00:00, 1.30MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 93.4kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:02<00:00, 555kB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 5.73MB/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='.', train=True,\n",
    "                                           download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST(root='.', train=False,\n",
    "                                          download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a9470c",
   "metadata": {},
   "source": [
    "Loading the datatset : \n",
    "\n",
    "Batching : loads multiple samples at once \n",
    "\n",
    "shuffles : shuffles the data to avoid order based pattern memorization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c404ccdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96927eb4",
   "metadata": {},
   "source": [
    "Too see some samples :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "848f77de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(im):\n",
    "    im = im/2 + 0.5 \n",
    "    npim = im.numpy()\n",
    "    plt.imshow(np.transpose(npim, (1, 2, 0)), cmap='grey') ## from C H W of tensors to H C W FOR matplotlib\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "815d0b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28])\n",
      "torch.Size([100])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: 8 6 9 9 1 7 3 8 3 6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get some random training images\n",
    "dataiter = iter(train_loader) # itearates over the data \n",
    "images, labels = next(dataiter) # returns a batch and labels\n",
    "\n",
    "print(images.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "# Show images\n",
    "show_image(torchvision.utils.make_grid(images[:10], nrow=5)) # takes first 8\n",
    "print('Labels:', ' '.join(f'{labels[j].item()}' for j in range(10))) # takes first labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38799420",
   "metadata": {},
   "source": [
    "Lets define the model now :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac64edf",
   "metadata": {},
   "source": [
    "Description :\n",
    "\n",
    "convolution layer will take (1, 28, 28) as input , evaluate every 3x3 patch and return an output of (32, 26 , 26)\n",
    "\n",
    "pool layer will take feature map of each 2x2 grid and take the strongest activation : \n",
    "input(32,26,26) and output (32,13,13)\n",
    "\n",
    "convolution layer 2 will generate 64 features from the 32, making more complex features \n",
    "input(32,13,13) and output (64, 11, 11)\n",
    "\n",
    "Will pool again to reduce to (64, 5 , 5)\n",
    "\n",
    "\n",
    "Now need to flatten to feed to a dense layer( linear), that will get a 1600 feats for each image, and generate 64 abstract feats that will then be fed to the output layer to \n",
    "classify among the 10 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5eb4b91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,32, kernel_size=3) # 1 channel in input , will output 32 feats \n",
    "        self.pool = nn.MaxPool2d(2,2) # will consider 2x2 patches\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)## will take 32 feats and generate 64\n",
    "        self.dense1 = nn.Linear(64*25, 64) #will take 64*25 feats and output 64\n",
    "        self.output = nn.Linear(64,10) ## will take the 64 abstract features and output probability of each label (softmax ish ?) \n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.pool(torch.relu(self.conv1(X))) # conv1 + relu + pool : conv generates 32 feats for \n",
    "        X = self.pool(torch.relu(self.conv2(X)))\n",
    "        X = X.view(-1 , 64*25) # flattens the tensor to have 1600 features for each image \n",
    "        X = torch.relu(self.dense1(X)) # relu in the sense layer\n",
    "        X = self.output(X)\n",
    "        return X \n",
    "        \n",
    "\n",
    "model = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69450d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  # for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89d0e00",
   "metadata": {},
   "source": [
    "Each parameter has a .grad attribute . When output and label is passed, loss func calculated the grad in .backward() and stores grads to .grads of each param "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772b7f23",
   "metadata": {},
   "source": [
    "The training :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ca8f121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.0042\n",
      "Epoch 2, Loss: 0.0026\n",
      "Epoch 3, Loss: 0.0043\n",
      "Epoch 4, Loss: 0.0015\n",
      "Epoch 5, Loss: 0.0011\n",
      "Epoch 6, Loss: 0.0038\n",
      "Epoch 7, Loss: 0.0054\n",
      "Epoch 8, Loss: 0.0036\n",
      "Epoch 9, Loss: 0.0008\n",
      "Epoch 10, Loss: 0.0002\n",
      "Epoch 11, Loss: 0.0000\n",
      "Epoch 12, Loss: 0.0012\n",
      "Epoch 13, Loss: 0.0083\n",
      "Epoch 14, Loss: 0.0033\n",
      "Epoch 15, Loss: 0.0015\n",
      "Epoch 16, Loss: 0.0018\n",
      "Epoch 17, Loss: 0.0030\n",
      "Epoch 18, Loss: 0.0013\n",
      "Epoch 19, Loss: 0.0026\n",
      "Epoch 20, Loss: 0.0022\n",
      "Epoch 21, Loss: 0.0021\n",
      "Epoch 22, Loss: 0.0024\n",
      "Epoch 23, Loss: 0.0016\n",
      "Epoch 24, Loss: 0.0011\n",
      "Epoch 25, Loss: 0.0033\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(25):\n",
    "    model.train()\n",
    "    running_loss = 0.0  # sum of losses for this epoch\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()  # add batch loss to total\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)  # average loss\n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b0fde3",
   "metadata": {},
   "source": [
    "Loss functions  knows the weights to calculate gradients for because pythorch has an inbuilt computation graph and loss just back tracks from that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad9c9493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 99.05%\n"
     ]
    }
   ],
   "source": [
    "model.eval() # evaluation mode , no changes to the model \n",
    "total = 0 \n",
    "correct = 0 \n",
    "wrong = 0\n",
    "with torch.no_grad():\n",
    "    for images , labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs,1) # in axis 1 (columns), we look for the highest value in tensor, stores index of the max\n",
    "        total += labels.size(0)\n",
    "        correct += (labels == predicted).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08423240",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
